{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2OHHmqSI6H62"
   },
   "outputs": [],
   "source": [
    "# Model-1-V5\n",
    "# - Load data set\n",
    "# - Remove NaN in short_desc\n",
    "# - Convert to lowercase\n",
    "# - Split the words using 1. ASCII character identification for english 2. Split by Space  3. wordninja\n",
    "# - Apply normlise\n",
    "# - Apply contractions/expansions \n",
    "# - Remove punctuations\n",
    "# - remove tags.\n",
    "# - remove special characters and digits.\n",
    "# - Stemming/Lemmatisation. \n",
    "# - Prepare the whole data set\n",
    "# - Calculate the similarity between every two bugreport for the same product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SE3nFMwg6XEB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2DH0GHXC6cQv"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://zenodo.org/record/400614/files/apache.csv?download=1',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2810,
     "status": "ok",
     "timestamp": 1576509943167,
     "user": {
      "displayName": "Alireza Ghasemieh",
      "photoUrl": "https://lh5.googleusercontent.com/--sm1QCsEBLM/AAAAAAAAAAI/AAAAAAAABII/jRVLOB4QFgQ/s64/photo.jpg",
      "userId": "15022417890264208659"
     },
     "user_tz": 300
    },
    "id": "Hdo7ksWKBANr",
    "outputId": "6fb03693-9307-430a-b55e-7adb6f21ae1c"
   },
   "outputs": [],
   "source": [
    "def remove_nan(df):\n",
    "    blanks = []\n",
    "    print(\"Before removing the NaN:\")\n",
    "    print(df.isnull().sum())\n",
    "    for x in df.itertuples():\n",
    "        if type(x.short_desc)!=str: # detect the NaN\n",
    "            blanks.append(x.Index)\n",
    "        elif not x.short_desc: # detect empty string\n",
    "            blanks.append(x.Index)\n",
    "    df.drop(blanks,inplace=True)\n",
    "    print(\"\\nAfter removing the NaN:\\n\",df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing the NaN:\n",
      "id                  0\n",
      "product             0\n",
      "component           0\n",
      "reporter            0\n",
      "bug_status          0\n",
      "resolution          0\n",
      "priority            0\n",
      "bug_severity        0\n",
      "version             0\n",
      "short_desc         58\n",
      "opendate            0\n",
      "dup_list        41531\n",
      "root_id         38016\n",
      "disc_id         38016\n",
      "dtype: int64\n",
      "\n",
      "After removing the NaN:\n",
      " id                  0\n",
      "product             0\n",
      "component           0\n",
      "reporter            0\n",
      "bug_status          0\n",
      "resolution          0\n",
      "priority            0\n",
      "bug_severity        0\n",
      "version             0\n",
      "short_desc          0\n",
      "opendate            0\n",
      "dup_list        41474\n",
      "root_id         37961\n",
      "disc_id         37961\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "remove_nan(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This module is responsible to preprocess the text inside a\n",
    "# data frame and gives a data frame with the processed text\n",
    "# It perform the following processes to the text:\n",
    "\n",
    "# Convert to lowercase\n",
    "# Split the words using 1. ASCII character identification for english 2. Split by Space  3. wordninja\n",
    "# Apply normlise\n",
    "# Apply contractions/expansions\n",
    "# Remove punctuations\n",
    "# remove tags.\n",
    "# remove special characters and digits.\n",
    "# Stemming/Lemmatisation.\n",
    "\n",
    "# The input is the df name and the textual attribute\n",
    "# name and the sample size from the df if you want to\n",
    "# conduct preprocess to a portion of the df\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# The following is the dependency list of the module\n",
    "\n",
    "# conda install -c conda-forge spacy\n",
    "# !pip install wordninja\n",
    "# !pip install normalise\n",
    "# !pip install pycontractions\n",
    "# conda install -c conda-forge spacy-lookups-data\n",
    "# !python -m spacy download en_core_web_lg\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "#import nltk\n",
    "# nltk.download('brown')\n",
    "# nltk.download('names')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('universal_tagset')\n",
    "import wordninja as nj  # for spliting the words in each documents\n",
    "import normalise as ns\n",
    "#from pycontractions import Contractions  # for expansion and contrations\n",
    "import contractions\n",
    "import re  # remove tags.\n",
    "\n",
    "\n",
    "def preprocessing(df, id, text_attribute_name):\n",
    "    # Extract and convert short_desc to string\n",
    "    row = df[df['id'] == int(id)]  # Extract a tuple from the dataframe\n",
    "    short_desc_to_string = row[text_attribute_name].to_string(index=False)  # Conver short_desc to string\n",
    "    short_desc_to_string = short_desc_to_string[1:]  # Remove the first space char from the begnning\n",
    "\n",
    "    # Convert to lowercase\n",
    "    short_desc_lowercase = short_desc_to_string.lower()\n",
    "\n",
    "    # Split the words using 1. ASCII character identification for english 2. Split by Space  3. wordninja\n",
    "    short_desc_splited = nj.split(short_desc_lowercase)\n",
    "\n",
    "    # Apply normlise\n",
    "    short_desc_normalised = ns.normalise(short_desc_splited, verbose=False)\n",
    "    short_desc_normalised_listToStr = ' '.join(map(str, short_desc_normalised))\n",
    "\n",
    "    # Apply contractions/expansions\n",
    "    short_desc_contract = contractions.fix(short_desc_normalised_listToStr)\n",
    "\n",
    "    # remove tags.\n",
    "    clean = re.compile('<.*?>')\n",
    "    short_desc_removed_tag = re.sub(clean, '', str(short_desc_contract))\n",
    "\n",
    "    # remove special characters and digits.\n",
    "    short_desc_removed_special_char = [re.sub(r\"[^a-zA-Z]+\", ' ', k) for k in str(short_desc_removed_tag).split(\"\\n\")]\n",
    "\n",
    "    # Remove Punctuations and Stop words\n",
    "    short_desc_doc = nlp(str(short_desc_removed_special_char))\n",
    "    short_desc_map = map(lambda token: token if (token.is_punct == False and token.is_stop == False) else None,\n",
    "                         short_desc_doc)\n",
    "    short_desc_list = list(short_desc_map)\n",
    "\n",
    "    # Convert list to string and remove one-character word\n",
    "    short_desc_string = \"\"\n",
    "    for element in short_desc_list:\n",
    "        if element is not None and len(element) > 1:\n",
    "            short_desc_string += str(element) + ' '\n",
    "    short_desc_string = short_desc_string[:-1]\n",
    "\n",
    "    # Stemming/Lemmatisation.\n",
    "    short_desc_lemmata = [token.lemma_ for token in nlp(short_desc_string)]\n",
    "    short_desc_preprocessed = ' '.join(map(str, short_desc_lemmata))\n",
    "\n",
    "    return short_desc_preprocessed\n",
    "\n",
    "\n",
    "def text_preprocessing(df, text_attribute_name, sample_number=None):\n",
    "    import time\n",
    "    import progressbar as pb\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        if sample_number is not None:\n",
    "            sample_size = sample_number  # Sample Size\n",
    "            progress = pb.ProgressBar(maxval=sample_size).start()\n",
    "        else:\n",
    "            dataset_length = len(df)\n",
    "            progress = pb.ProgressBar(maxval=dataset_length).start()\n",
    "        progvar = 0\n",
    "        processed_string_list = []\n",
    "\n",
    "        for x in df.itertuples():\n",
    "            string = preprocessing(df, x.id, text_attribute_name)\n",
    "            processed_string_list.append((x.id, string))\n",
    "\n",
    "            # Show the progress in the output\n",
    "            progress.update(progvar + 1)\n",
    "            progvar += 1\n",
    "\n",
    "            # Terminate the process when reach to sample size\n",
    "            if (sample_number is not None) and (progvar >= sample_size):\n",
    "                break\n",
    "        # Convert list to dataframe\n",
    "        text = text_attribute_name + '_processed'\n",
    "        processed_string_df = pd.DataFrame(processed_string_list, columns=['id', text])\n",
    "        del processed_string_list\n",
    "\n",
    "        # Join two df\n",
    "        processed_data_df = pd.merge(processed_string_df, df, on='id')\n",
    "    \n",
    "        # show the time of process\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "        return processed_data_df\n",
    "    except:\n",
    "        print('There is a problem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import text_processing as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (99 of 100) |###################### | Elapsed Time: 0:00:09 ETA:   0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 10.210246562957764 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>short_desc_processed</th>\n",
       "      <th>product</th>\n",
       "      <th>component</th>\n",
       "      <th>reporter</th>\n",
       "      <th>bug_status</th>\n",
       "      <th>resolution</th>\n",
       "      <th>priority</th>\n",
       "      <th>bug_severity</th>\n",
       "      <th>version</th>\n",
       "      <th>short_desc</th>\n",
       "      <th>opendate</th>\n",
       "      <th>dup_list</th>\n",
       "      <th>root_id</th>\n",
       "      <th>disc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>testing book zee la setup log</td>\n",
       "      <td>Log4j -</td>\n",
       "      <td>Layout</td>\n",
       "      <td>bugzilla</td>\n",
       "      <td>CLOS</td>\n",
       "      <td>FIXE</td>\n",
       "      <td>P3</td>\n",
       "      <td>nor</td>\n",
       "      <td>unspe</td>\n",
       "      <td>Just testing the Boogzeela setup for log4j</td>\n",
       "      <td>2001-01-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>file share</td>\n",
       "      <td>Log4j -</td>\n",
       "      <td>Appender</td>\n",
       "      <td>matthew_scully</td>\n",
       "      <td>RESO</td>\n",
       "      <td>FIXE</td>\n",
       "      <td>P3</td>\n",
       "      <td>nor</td>\n",
       "      <td>unspe</td>\n",
       "      <td>file sharing</td>\n",
       "      <td>2001-01-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>need hooks user define error handling</td>\n",
       "      <td>Apache h</td>\n",
       "      <td>All</td>\n",
       "      <td>greenrd</td>\n",
       "      <td>CLOS</td>\n",
       "      <td>WONT</td>\n",
       "      <td>P5</td>\n",
       "      <td>enh</td>\n",
       "      <td>2.0-H</td>\n",
       "      <td>Need hooks for user-defined error handling</td>\n",
       "      <td>2001-01-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>error forget layout</td>\n",
       "      <td>Log4j -</td>\n",
       "      <td>Layout</td>\n",
       "      <td>richard.mccarthy</td>\n",
       "      <td>RESO</td>\n",
       "      <td>INVA</td>\n",
       "      <td>P1</td>\n",
       "      <td>nor</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Error in the formatting of the (%F:%L) layout ...</td>\n",
       "      <td>2001-01-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>call category error object throw able thro</td>\n",
       "      <td>Log4j -</td>\n",
       "      <td>Other</td>\n",
       "      <td>woge</td>\n",
       "      <td>RESO</td>\n",
       "      <td>FIXE</td>\n",
       "      <td>P3</td>\n",
       "      <td>nor</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Calling Category.error(Object, Throwable) thro...</td>\n",
       "      <td>2001-01-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                        short_desc_processed   product component  \\\n",
       "0   2               testing book zee la setup log   Log4j -    Layout   \n",
       "1   3                                  file share   Log4j -  Appender   \n",
       "2  22       need hooks user define error handling  Apache h       All   \n",
       "3  27                         error forget layout   Log4j -    Layout   \n",
       "4  29  call category error object throw able thro   Log4j -     Other   \n",
       "\n",
       "           reporter bug_status resolution priority bug_severity version  \\\n",
       "0          bugzilla       CLOS       FIXE       P3          nor   unspe   \n",
       "1    matthew_scully       RESO       FIXE       P3          nor   unspe   \n",
       "2           greenrd       CLOS       WONT       P5          enh   2.0-H   \n",
       "3  richard.mccarthy       RESO       INVA       P1          nor     1.0   \n",
       "4              woge       RESO       FIXE       P3          nor     1.0   \n",
       "\n",
       "                                          short_desc    opendate dup_list  \\\n",
       "0         Just testing the Boogzeela setup for log4j  2001-01-08      NaN   \n",
       "1                                       file sharing  2001-01-08      NaN   \n",
       "2         Need hooks for user-defined error handling  2001-01-09      NaN   \n",
       "3  Error in the formatting of the (%F:%L) layout ...  2001-01-10      NaN   \n",
       "4  Calling Category.error(Object, Throwable) thro...  2001-01-11      NaN   \n",
       "\n",
       "   root_id  disc_id  \n",
       "0      NaN      NaN  \n",
       "1      NaN      NaN  \n",
       "2      NaN      NaN  \n",
       "3      NaN      NaN  \n",
       "4      NaN      NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data_df = text_preprocessing(data,\"short_desc\",100)\n",
    "processed_data_df.to_csv('~/Desktop/Google-Drive/Colab Notebooks/processed_data_df.csv',index=False)\n",
    "processed_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_data_df = pd.read_csv('~/Desktop/Google-Drive/Colab Notebooks/Prepared_data/processed_data_df.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_id(df,original_df = data):\n",
    "    length = len(df)\n",
    "    for tup in df.itertuples():\n",
    "        df.loc[tup.Index,'id1'] = original_df.iloc[tup.id1]['id']\n",
    "        df.loc[tup.Index,'id2'] = original_df.iloc[tup.id2]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_diff_product_score(df,original_df = data):\n",
    "    for tup in df.itertuples():\n",
    "        product1 = original_df.loc[lambda df: df['id'] == tup.id1,'product'].array[0]\n",
    "        product2 = original_df.loc[lambda df: df['id'] == tup.id2,'product'].array[0]\n",
    "        if product1 != product2:\n",
    "            df.drop([tup.Index],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model-1: Similarity Score - Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "It9SV878P1JL",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 29.921808004379272 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2100, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate similarity score Using word2vec\n",
    "import progressbar as pb\n",
    "start_time = time.time()\n",
    "\n",
    "# Convert short_desc str to nlp format to fasten the computation process\n",
    "processed_data_nlp = []\n",
    "\n",
    "for tup in processed_data_df.itertuples():\n",
    "    processed_data_nlp.append((tup.id,tup.product,nlp(tup.short_desc)))\n",
    "    \n",
    "processed_data_nlp_df = pd.DataFrame(processed_data_nlp, columns=['id','product','short_desc'])\n",
    "processed_data_nlp_df.head()\n",
    "del processed_data_nlp\n",
    "\n",
    "# Calculate the similarity score\n",
    "similarities_score_list = []\n",
    "\n",
    "for doc1 in processed_data_nlp_df.itertuples():\n",
    "    for doc2 in processed_data_nlp_df.itertuples():\n",
    "        if (doc1.id < doc2.id) and (doc1.product == doc2.product): # if two bug reports belong to the same product then check the similarity\n",
    "            similarity_score = doc1.short_desc.similarity(doc2.short_desc)\n",
    "            similarities_score_list.append((doc1.id,doc2.id,similarity_score))         \n",
    "\n",
    "# Sort based on the score and \n",
    "sorted_similarities_score = sorted(similarities_score_list, key=lambda tup: tup[2], reverse=True)\n",
    "del similarities_score_list\n",
    "\n",
    "#convert to dataframe\n",
    "word2vec_similarities_score_df = pd.DataFrame(sorted_similarities_score, columns=['id1','id2','score'] )\n",
    "del sorted_similarities_score\n",
    "\n",
    "word2vec_similarities_score_df.to_csv('~/Desktop/Google-Drive/Colab Notebooks/word2vec_similarities_score_df.csv',index=False)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time)) # show the time of process\n",
    "word2vec_similarities_score_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jw08fd9KAFuA"
   },
   "source": [
    "Model-2: Similarity Score - tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 12.266091585159302 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2100, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate similarity score Using TF-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "start_time = time.time()\n",
    "\n",
    "X_train = processed_data_df['short_desc']\n",
    "\n",
    "# Vectorization \n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(X_train) # remember to use the original X_train set\n",
    "\n",
    "# Calculate the similarity score\n",
    "tfidf_cosine_similarities = linear_kernel(tfidf)\n",
    "\n",
    "shape = tfidf_cosine_similarities.shape[0]\n",
    "\n",
    "tfidf_cosine_similarities_list = []\n",
    "\n",
    "for index1 in range(shape):\n",
    "    for index2 in range(shape):\n",
    "        if index2 > index1:\n",
    "            tfidf_cosine_similarities_list.append([index1,index2,tfidf_cosine_similarities[index1,index2]]) \n",
    "    \n",
    "# Sort the score list\n",
    "tfidf_cosine_similarities_sort = sorted(tfidf_cosine_similarities_list, key=lambda tup: tup[2], reverse=True)\n",
    "del tfidf_cosine_similarities_list\n",
    "\n",
    "#Conver to dataframe\n",
    "tfidf_cosine_similarities_score_df = pd.DataFrame(tfidf_cosine_similarities_sort, columns=['id1','id2','score'])\n",
    "del tfidf_cosine_similarities_sort\n",
    "\n",
    "# Convert index to id\n",
    "index_to_id(tfidf_cosine_similarities_score_df, data)\n",
    "\n",
    "# Remove record of diff product\n",
    "remove_diff_product_score(tfidf_cosine_similarities_score_df)\n",
    "\n",
    "tfidf_cosine_similarities_score_df.to_csv('~/Desktop/Google-Drive/Colab Notebooks/tfidf_cosine_similarities_score_df.csv',index=False)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time)) # show the time of process\n",
    "\n",
    "tfidf_cosine_similarities_score_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model-3: Similarity Score - BM24F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 12.154354095458984 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2100, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate similarity score Using BM25\n",
    "from rank_bm25 import BM25Okapi\n",
    "start_time = time.time()\n",
    "\n",
    "# preprocess - tokenize the short_desc to token\n",
    "processed_corpus_list = []\n",
    "for x in processed_data_df.itertuples():\n",
    "    short_desc_splited = x.short_desc.split(\" \")\n",
    "    processed_corpus_list.append(short_desc_splited)\n",
    "    \n",
    "# Create a MB24 Object with the corpus\n",
    "bm25 = BM25Okapi(processed_corpus_list)\n",
    "\n",
    "# Calculat the similarity score for all bug reports\n",
    "BM24_similarity_score_list = []\n",
    "\n",
    "for x in processed_data_df.itertuples():\n",
    "    query = processed_corpus_list[x.Index]\n",
    "    doc_scores = bm25.get_scores(query)\n",
    "    BM24_similarity_score_list.append([x.Index,doc_scores])\n",
    "\n",
    "# Reformating \n",
    "shape = doc_scores.shape[0]\n",
    "BM24_similarity_score_list_2 = []\n",
    "\n",
    "for index1 in range(shape):\n",
    "    for index2 in range(shape):\n",
    "        if index2 > index1:\n",
    "            BM24_similarity_score_list_2.append([index1,index2,BM24_similarity_score_list[index1][1][index2]])  \n",
    "            \n",
    "del BM24_similarity_score_list\n",
    "\n",
    "# Sort the score list\n",
    "BM24_similarity_score_sort = sorted(BM24_similarity_score_list_2, key=lambda tup: tup[2], reverse=True)\n",
    "del BM24_similarity_score_list_2\n",
    "\n",
    "#Conver to dataframe\n",
    "BM24_similarity_score_sort_df = pd.DataFrame(BM24_similarity_score_sort, columns=['id1','id2','score'])\n",
    "del BM24_similarity_score_sort\n",
    "\n",
    "# Convert index to id\n",
    "index_to_id(BM24_similarity_score_sort_df)\n",
    "\n",
    "# Remove record of diff product\n",
    "remove_diff_product_score(BM24_similarity_score_sort_df)\n",
    "\n",
    "BM24_similarity_score_sort_df.to_csv('~/Desktop/Google-Drive/Colab Notebooks/BM24_similarity_score_sort_df.csv',index=False)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time)) # show the time of process\n",
    "\n",
    "BM24_similarity_score_sort_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessment the Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_org = pd.read_csv('https://zenodo.org/record/400614/files/apache.relations.csv?download=1',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_df = word2vec_similarities_score_df\n",
    "tfidf_df = tfidf_cosine_similarities_score_df\n",
    "BM24_df = BM24_similarity_score_sort_df\n",
    "\n",
    "duplicate_score_table_list = []\n",
    "\n",
    "for tup in duplicate_org.itertuples():\n",
    "    word2vec_score = word2vec_df.loc[(word2vec_df['id1'] == tup.id) & (word2vec_df['id2'] == tup.dup),'score'].array[0]\n",
    "    tfidf_score = tfidf_df.loc[(tfidf_df['id1'] == tup.id) & (tfidf_df['id2'] == tup.dup),'score'].array[0]\n",
    "    BM24F_score = BM24_df.loc[(BM24_df['id1'] == tup.id) & (BM24_df['id2'] == tup.dup),'score'].array[0]\n",
    "\n",
    "    short_desc1 = data_org.loc[lambda df: df['id'] == tup.id,'short_desc'].array[0]\n",
    "    short_desc2 = data_org.loc[lambda df: df['id'] == tup.dup,'short_desc'].array[0]\n",
    "    \n",
    "    duplicate_score_table_list.append([tup.id,tup.dup,word2vec_score,tfidf_score,BM24F_score,short_desc1,short_desc2])\n",
    "\n",
    "duplicate_score_table_df = pd.DataFrame(duplicate_score_table_list, columns=['id1','dup','word2vec_score','tfidf_score','BM24F_score','short_desc1','short_desc2'])\n",
    "del duplicate_score_table_list\n",
    "\n",
    "duplicate_score_table_df.to_csv('~/Desktop/Google-Drive/Colab Notebooks/duplicate_score_table_df.csv',index=False)\n",
    "\n",
    "\n",
    "# print(\"short_desc1\\n\",short_desc1)\n",
    "# print(\"short_desc2\\n\",short_desc2)\n",
    "# print(\"word2vec_score:\\t\",word2vec_score)\n",
    "# print(\"tfidf_score:\\t\",tfidf_score)\n",
    "# print(\"BM24F_score:\\t\",BM24F_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Model-1_V04.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
