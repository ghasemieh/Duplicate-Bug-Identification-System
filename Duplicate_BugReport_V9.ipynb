{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Duplicate_BugReport_V9.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ghasemieh/Duplicate-Bug-Identification-System/blob/master/Duplicate_BugReport_V9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g4Y20uWiqC_o",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import progressbar as pb\n",
        "import time\n",
        "import warnings\n",
        "import text_processing as tp\n",
        "warnings.filterwarnings('ignore')\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wcgC4k6LqC_t",
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": true,
        "colab": {},
        "outputId": "02a8ac35-9238-4f69-839c-ba29146e440d"
      },
      "source": [
        "# Import dataset\n",
        "data = pd.read_csv('TestData2500.csv',sep=',')\n",
        "# Remove nan from the main dataset\n",
        "tp.remove_nan(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Remove NA\n",
            "Before removing the NaN:\n",
            "id                 0\n",
            "product            0\n",
            "component          0\n",
            "reporter           0\n",
            "bug_status         0\n",
            "resolution         0\n",
            "priority           0\n",
            "bug_severity       0\n",
            "version            0\n",
            "short_desc         1\n",
            "opendate           0\n",
            "dup_list        1499\n",
            "root_id         1406\n",
            "disc_id         1406\n",
            "dtype: int64\n",
            "\n",
            "After removing the NaN:\n",
            " id                 0\n",
            "product            0\n",
            "component          0\n",
            "reporter           0\n",
            "bug_status         0\n",
            "resolution         0\n",
            "priority           0\n",
            "bug_severity       0\n",
            "version            0\n",
            "short_desc         0\n",
            "opendate           0\n",
            "dup_list        1498\n",
            "root_id         1405\n",
            "disc_id         1405\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YSGUykkOqC_w",
        "pycharm": {
          "is_executing": false
        },
        "colab": {},
        "outputId": "6641d147-337e-4343-e17f-47aac791e580"
      },
      "source": [
        "# Preprocessing the short_desc\n",
        "# processed_data_df = tp.text_preprocessing(data,\"short_desc\")\n",
        "# # Save to file to save time\n",
        "# processed_data_df.to_csv('processed_TestData2500_df.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rN/A% (0 of 1594) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Preprocessing the text\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 99% (1592 of 1594) |################### | Elapsed Time: 0:00:42 ETA:   0:00:00"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Remove NA\n",
            "Before removing the NaN:\n",
            "id                         0\n",
            "short_desc_processed       0\n",
            "product                    0\n",
            "component                  0\n",
            "reporter                   0\n",
            "bug_status                 0\n",
            "resolution                 0\n",
            "priority                   0\n",
            "bug_severity               0\n",
            "version                    0\n",
            "short_desc                 0\n",
            "opendate                   0\n",
            "dup_list                1498\n",
            "root_id                 1405\n",
            "disc_id                 1405\n",
            "dtype: int64\n",
            "\n",
            "After removing the NaN:\n",
            " id                         0\n",
            "short_desc_processed       0\n",
            "product                    0\n",
            "component                  0\n",
            "reporter                   0\n",
            "bug_status                 0\n",
            "resolution                 0\n",
            "priority                   0\n",
            "bug_severity               0\n",
            "version                    0\n",
            "short_desc                 0\n",
            "opendate                   0\n",
            "dup_list                1498\n",
            "root_id                 1405\n",
            "disc_id                 1405\n",
            "dtype: int64\n",
            "Text preprocessing --- 42.19237446784973 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BerhLMLBqC_z",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "# Load prosecced df from file to sasve time\n",
        "processed_data_df = pd.read_csv('processed_TestData2500_df.csv',sep=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KlpjDsOiqC_1",
        "pycharm": {
          "is_executing": false
        },
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# Model-1: Similarity Score - Word2vec -------------------------------------------------------------------------\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "execution_count_word2vec = 0\n",
        "processed_data_nlp_df = []\n",
        "\n",
        "# Convert short_desc str to nlp format to fasten the computation process \n",
        "def word2vec_preprocess(df):\n",
        "    print('Convert short_desc str to nlp format')\n",
        "    sample_size = len(df)\n",
        "    progress = pb.ProgressBar(maxval = sample_size).start()\n",
        "    progvar  = 0\n",
        "    processed_data_nlp = []\n",
        "    for tup in df.itertuples():\n",
        "        processed_data_nlp.append((tup.id,tup.product,nlp(tup.short_desc_processed))) \n",
        "        progress.update(progvar + 1)\n",
        "        progvar += 1\n",
        "    global processed_data_nlp_df\n",
        "    processed_data_nlp_df = pd.DataFrame(processed_data_nlp, columns=['id','product','short_desc_processed'])\n",
        "    global execution_count_word2vec\n",
        "    execution_count_word2vec += 1\n",
        "\n",
        "# Calculate the cosine similarity score\n",
        "def word2vec_similarity(id,df):\n",
        "    if execution_count_word2vec == 0:\n",
        "        word2vec_preprocess(df)\n",
        "    similarities_score_list = []\n",
        "    product_main = processed_data_nlp_df.loc[lambda df: df['id'] == id,'product'].array[0]\n",
        "    short_desc_processed_main = processed_data_nlp_df.loc[lambda df: df['id'] == id,'short_desc_processed'].array[0]\n",
        "    for doc in processed_data_nlp_df.itertuples():\n",
        "        product_other = processed_data_nlp_df.loc[lambda df: df['id'] == doc.id,'product'].array[0]\n",
        "        if product_main == product_other:\n",
        "            similarity_score = doc.short_desc_processed.similarity(short_desc_processed_main)\n",
        "            similarities_score_list.append((doc.id,similarity_score))\n",
        "    #convert to dataframe\n",
        "    word2vec_similarities_score_df = pd.DataFrame(similarities_score_list, columns=['id','word2vec_score'] )\n",
        "    word2vec_similarities_score_df = word2vec_similarities_score_df.reset_index(drop=True)\n",
        "    return word2vec_similarities_score_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-gU1hrZ1qC_3",
        "pycharm": {
          "is_executing": true
        },
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# Model-2: Similarity Score - TF-idf ----------------------------------------------------------------------------\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "tfidf_cosine_similarities = []\n",
        "execution_count_tfidf = 0\n",
        "\n",
        "def tfidf_preprocess(df):\n",
        "    X_train = df['short_desc_processed']\n",
        "    print('TF-idf Vectorization and similarity score computation')\n",
        "    # Vectorization\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf = vectorizer.fit_transform(X_train)\n",
        "    # Calculate the cosine similarity score\n",
        "    global tfidf_cosine_similarities\n",
        "    tfidf_cosine_similarities = linear_kernel(tfidf)\n",
        "    global shape_tfidf\n",
        "    shape_tfidf = tfidf_cosine_similarities.shape[0]\n",
        "    print('TF-idf preprocess done')\n",
        "    global execution_count_tfidf\n",
        "    execution_count_tfidf += 1\n",
        "\n",
        "def tfidf_similarities(id,df):\n",
        "    if execution_count_tfidf == 0:\n",
        "        tfidf_preprocess(df)\n",
        "    index_main   = df.loc[lambda df: df['id'] == id].index.array[0]\n",
        "    product_main = df.loc[lambda df: df['id'] == id,'product'].array[0]\n",
        "    tfidf_cosine_similarities_list = []\n",
        "    for index_other in range(shape_tfidf):\n",
        "        id_other      = df.iloc[index_other]['id']\n",
        "        product_other = df.iloc[index_other]['product']\n",
        "        if product_main == product_other:\n",
        "            tfidf_cosine_similarities_list.append([id_other,tfidf_cosine_similarities[index_main,index_other]])\n",
        "    #Conver to dataframe\n",
        "    tfidf_cosine_similarities_score_df = pd.DataFrame(tfidf_cosine_similarities_list, columns=['id','tfidf_score'])\n",
        "    tfidf_cosine_similarities_score_df = tfidf_cosine_similarities_score_df.reset_index(drop=True)\n",
        "    return tfidf_cosine_similarities_score_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z5wlDGp3qC_6",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "# Model-3: Similarity Score - BM25F -----------------------------------------------------------------------------\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "processed_corpus_list = []\n",
        "bm25 = []\n",
        "execution_count_bm25 = 0\n",
        "\n",
        "# preprocess - tokenize the short_desc to token\n",
        "def bm25_preprocess(df):\n",
        "    print('preprocess - tokenize the short_desc to token')\n",
        "    sample_size = len(df)\n",
        "    global processed_corpus_list\n",
        "    processed_corpus_list = []\n",
        "    for x in df.itertuples():\n",
        "        short_desc_splited = x.short_desc_processed.split(\" \")\n",
        "        processed_corpus_list.append(short_desc_splited)\n",
        "    # Create a MB24 Object with the corpus\n",
        "    global bm25\n",
        "    bm25 = BM25Okapi(processed_corpus_list)\n",
        "    global execution_count_bm25\n",
        "    execution_count_bm25 += 1\n",
        "    \n",
        "# Calculate the similarity score\n",
        "def bm25_similarity(id,df):\n",
        "    if execution_count_bm25 == 0:\n",
        "        bm25_preprocess(df)\n",
        "    index_main    = df.loc[lambda df: df['id'] == id].index.array[0]\n",
        "    product_main  = df.loc[lambda df: df['id'] == id,'product'].array[0]\n",
        "    query         = processed_corpus_list[index_main]\n",
        "    doc_scores    = bm25.get_scores(query)\n",
        "    doc_scores_df = pd.DataFrame(doc_scores, columns=['bm25_score'])\n",
        "    # add id to the score list and remove unsimiliar product\n",
        "    blanks = []\n",
        "    for x in doc_scores_df.itertuples():\n",
        "        id_other      = df.iloc[x.Index]['id']\n",
        "        product_other = df.iloc[x.Index]['product']\n",
        "        # add id to the score list\n",
        "        doc_scores_df.loc[x.Index,'id'] = id_other\n",
        "        if product_main != product_other:\n",
        "            blanks.append(x.Index)    \n",
        "    doc_scores_df.drop(blanks,inplace=True)   \n",
        "    doc_scores_df = doc_scores_df.reset_index(drop = True)  \n",
        "    return doc_scores_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1z8EslEyqC_8"
      },
      "source": [
        "# Assessment the Model Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ikHJjrkvqC_9",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# Assessment the Model Accuracy -------------------------------------------------------------------------------\n",
        "# duplicate_df = pd.read_csv('https://zenodo.org/record/400614/files/apache.relations.csv?download=1',sep=',')\n",
        "duplicate_df = pd.read_csv('duplicate_df.csv',sep=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_qFg56-HqC__",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# Calculate the similarity scores and return the first n top scoes\n",
        "def similarity_score(id, df,top_n):\n",
        "    word2vec_similarity_df = word2vec_similarity(id, df).sort_values(by=['word2vec_score'],ascending=False).head(top_n)\n",
        "    tfidf_similarity_df    = tfidf_similarities(id, df).sort_values(by=['tfidf_score'],ascending=False).head(top_n)\n",
        "    bm25_similarity_df     = bm25_similarity(id, df).sort_values(by=['bm25_score'],ascending=False).head(top_n)    \n",
        "    return word2vec_similarity_df, tfidf_similarity_df, bm25_similarity_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lgayCI1DqDAC",
        "outputId": "f01c78d2-a083-44ac-f740-744535e96df2",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# Calculate the similarity score for the reported duplicated bug report\n",
        "start_time  = time.time()\n",
        "sample_size = len(duplicate_df)\n",
        "progress    = pb.ProgressBar(maxval = sample_size).start()\n",
        "progvar     = 0\n",
        "duplicated_similarity_score_list = []\n",
        "\n",
        "for tup in duplicate_df.itertuples():\n",
        "    word2vec_similarity_df, tfidf_similarity_df, bm25_similarity_df = similarity_score(tup.id, processed_data_df,20)\n",
        "    duplicated_similarity_score_list.append([tup.id,tup.dup,word2vec_similarity_df,tfidf_similarity_df,bm25_similarity_df])\n",
        "    progress.update(progvar + 1)\n",
        "    progvar += 1\n",
        "    \n",
        "print(\"-Total- %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100% (83 of 83) |########################| Elapsed Time: 0:03:44 ETA:  00:00:00"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-Total- 224.28654527664185 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4DtDQ8rhqDAF",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# Save the list to a file since it takes 8 hours to create it\n",
        "with open(\"duplicated_similarity_score_list_2500.txt\", \"wb\") as fp:   #Pickling\n",
        "    pickle.dump(duplicated_similarity_score_list, fp)\n",
        "\n",
        "# # Read the list from a file \n",
        "with open(\"duplicated_similarity_score_list_2500.txt\", \"rb\") as fp:   # Unpickling\n",
        "    duplicated_similarity_score_list = pickle.load(fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TdwV5V47k5A",
        "colab_type": "text"
      },
      "source": [
        "## Calculate the Recall Rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JKSY0w_cqJXg",
        "colab": {}
      },
      "source": [
        "# Calculate the Recall rate\n",
        "def recall_rate_calculation(name_of_algorthem):\n",
        "    if name_of_algorthem == 'word2vec':\n",
        "        select_algorithm = 2\n",
        "        print('word2vec result:')\n",
        "    elif name_of_algorthem == 'tfidf':\n",
        "        select_algorithm = 3\n",
        "        print('TF-idf result:')\n",
        "    elif name_of_algorthem == 'bm25':\n",
        "        select_algorithm = 4\n",
        "        print('bm25 result:')\n",
        "    else:\n",
        "        return \"Wrong selection\"\n",
        "    found_counter = 0\n",
        "    not_found_counter = 0\n",
        "    for x in duplicated_similarity_score_list:\n",
        "        dup = x[1]\n",
        "        df = x[select_algorithm]\n",
        "        if df.loc[df['id'] == dup].empty:\n",
        "            not_found_counter +=1\n",
        "        else:\n",
        "            found_counter +=1\n",
        "    print('Num of duplicated report found: ',found_counter)\n",
        "    print('Num of duplicated report not found: ',not_found_counter)\n",
        "    print('Recall (TP/TP+FN): ', round(found_counter/len(duplicated_similarity_score_list)*100,2),'%\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d8uPULPPqa4c",
        "colab": {},
        "outputId": "8e293d6a-8221-4976-ffbb-9f46f32a6d7e"
      },
      "source": [
        "# Check the results\n",
        "recall_rate_calculation('word2vec')\n",
        "recall_rate_calculation('tfidf')\n",
        "recall_rate_calculation('bm25')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word2vec result:\n",
            "Num of duplicated report found:  48\n",
            "Num of duplicated report not found:  35\n",
            "Recall (TP/TP+FN):  57.83 %\n",
            "\n",
            "TF-idf result:\n",
            "Num of duplicated report found:  51\n",
            "Num of duplicated report not found:  32\n",
            "Recall (TP/TP+FN):  61.45 %\n",
            "\n",
            "bm25 result:\n",
            "Num of duplicated report found:  50\n",
            "Num of duplicated report not found:  33\n",
            "Recall (TP/TP+FN):  60.24 %\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34FwRGi67k5F",
        "colab_type": "text"
      },
      "source": [
        "## Calculate the MRR \n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2P03tLP7k5F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate the MRR\n",
        "def MRR_rate_calculation(name_of_algorthem):\n",
        "    if name_of_algorthem == 'word2vec':\n",
        "        select_algorithm = 2\n",
        "        print('word2vec result:')\n",
        "    elif name_of_algorthem == 'tfidf':\n",
        "        select_algorithm = 3\n",
        "        print('TF-idf result:')\n",
        "    elif name_of_algorthem == 'bm25':\n",
        "        select_algorithm = 4\n",
        "        print('bm25 result:')\n",
        "    else:\n",
        "        return \"Wrong selection\"\n",
        "    found_counter = 0\n",
        "    not_found_counter = 0\n",
        "    revers_sum = 0\n",
        "    for x in duplicated_similarity_score_list:\n",
        "        dup = x[1]\n",
        "        df = x[select_algorithm].reset_index(drop=True)\n",
        "        if df.loc[df['id'] == dup].empty:\n",
        "            not_found_counter +=1\n",
        "        else:\n",
        "            found_counter +=1\n",
        "            for tup in df.itertuples():\n",
        "                if int(tup.id) == dup:\n",
        "                    if tup.Index ==0 :\n",
        "                        revers_sum += 1/(tup.Index+1)\n",
        "                    else:\n",
        "                        revers_sum += 1/(tup.Index)\n",
        "                \n",
        "    print('Num of duplicated report found: ',found_counter)\n",
        "    print('Num of duplicated report not found: ',not_found_counter)\n",
        "    print('MRR_rate: ', round((1/len(duplicated_similarity_score_list))*revers_sum,5),'\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i52KoFL67k5I",
        "colab_type": "code",
        "colab": {},
        "outputId": "b0034758-6d9e-45a5-9f56-4c98018ad3fe"
      },
      "source": [
        "MRR_rate_calculation('word2vec')\n",
        "MRR_rate_calculation('tfidf')\n",
        "MRR_rate_calculation('bm25')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word2vec result:\n",
            "Num of duplicated report found:  48\n",
            "Num of duplicated report not found:  35\n",
            "MRR_rate:  0.3138 \n",
            "\n",
            "TF-idf result:\n",
            "Num of duplicated report found:  51\n",
            "Num of duplicated report not found:  32\n",
            "MRR_rate:  0.35534 \n",
            "\n",
            "bm25 result:\n",
            "Num of duplicated report found:  50\n",
            "Num of duplicated report not found:  33\n",
            "MRR_rate:  0.34919 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9Jy_whV7k5L",
        "colab_type": "text"
      },
      "source": [
        "## Calculate the MAP\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gZJupJD7k5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate the MAP\n",
        "def MAP_rate_calculation(name_of_algorthem, threshold):\n",
        "    if name_of_algorthem == 'word2vec':\n",
        "        select_algorithm = 2\n",
        "        print('word2vec result:')\n",
        "    elif name_of_algorthem == 'tfidf':\n",
        "        select_algorithm = 3\n",
        "        print('TF-idf result:')\n",
        "    elif name_of_algorthem == 'bm25':\n",
        "        select_algorithm = 4\n",
        "        print('bm25 result:')\n",
        "    else:\n",
        "        return \"Wrong selection\"\n",
        "    found_counter = 0\n",
        "    not_found_counter = 0\n",
        "    sum_overall_AP = 0\n",
        "    overall_AP = 0\n",
        "    for x in duplicated_similarity_score_list:\n",
        "        dup = x[1]\n",
        "        df = x[select_algorithm].reset_index(drop=True)\n",
        "        \n",
        "        if df.loc[df['id'] == dup].empty:\n",
        "            not_found_counter +=1\n",
        "        else:\n",
        "            num_true_positive = 1\n",
        "            sumAP = 0\n",
        "            for tup in df.itertuples():\n",
        "                if int(tup.id) == dup:\n",
        "                    found_counter +=1\n",
        "                    if tup.Index ==0 :\n",
        "                        sumAP += num_true_positive/(tup.Index+1)\n",
        "                    else:\n",
        "                        sumAP += num_true_positive/(tup.Index)\n",
        "                num_true_positive += 1 \n",
        "            overall_AP = (1/num_true_positive) * sumAP\n",
        "        sum_overall_AP += overall_AP\n",
        "                \n",
        "    print('Num of duplicated report found: ',found_counter)\n",
        "    print('Num of duplicated report not found: ',not_found_counter)\n",
        "    print('MAP_rate: ',round(sum_overall_AP/len(duplicated_similarity_score_list)*100,5), '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "520L5aoT7k5P",
        "colab_type": "code",
        "colab": {},
        "outputId": "724845c2-0142-4f8e-ae82-a1a39a9ede93"
      },
      "source": [
        "MAP_rate_calculation('word2vec',0.5)\n",
        "MAP_rate_calculation('tfidf',0.3)\n",
        "MAP_rate_calculation('bm25',0.5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word2vec result:\n",
            "Num of duplicated report found:  48\n",
            "Num of duplicated report not found:  35\n",
            "MAP_rate:  6.30397 \n",
            "\n",
            "TF-idf result:\n",
            "Num of duplicated report found:  51\n",
            "Num of duplicated report not found:  32\n",
            "MAP_rate:  6.84587 \n",
            "\n",
            "bm25 result:\n",
            "Num of duplicated report found:  50\n",
            "Num of duplicated report not found:  33\n",
            "MAP_rate:  6.70286 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYpkm2oH7k5T",
        "colab_type": "text"
      },
      "source": [
        "## TF-idf PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkJ52lZJ7k5U",
        "colab_type": "text"
      },
      "source": [
        "### Feature Transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM-kY5wN7k5U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lkWvyAE7k5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spark = SparkSession.builder.appName('nlp').getOrCreate()\n",
        "data = spark.read.csv(\"processed_TestData2500_df.csv\",inferSchema=True,header=True,sep=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVegUF9A7k5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import Tokenizer,StopWordsRemover, CountVectorizer,IDF,StringIndexer\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"short_desc_processed\", outputCol=\"token_short_desc_processed\")\n",
        "count_vec = CountVectorizer(inputCol='token_short_desc_processed',outputCol='c_vec')\n",
        "idf = IDF(inputCol=\"c_vec\", outputCol=\"tf_idf\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US54j3hL7k5d",
        "colab_type": "text"
      },
      "source": [
        "### Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXtXPsY27k5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml import Pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MItEUaDL7k5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_prep_pipe = Pipeline(stages=[tokenizer,count_vec,idf])\n",
        "cleaner = data_prep_pipe.fit(data)\n",
        "clean_data = cleaner.transform(data)\n",
        "clean_data = clean_data.select(['id','product','tf_idf'])\n",
        "tfidf = clean_data.select(['tf_idf'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tea34v97k5j",
        "colab_type": "code",
        "colab": {},
        "outputId": "1223e3d8-bb7a-4307-da1b-4cdd27f80863"
      },
      "source": [
        "clean_data.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------+--------------------+\n",
            "| id| product|              tf_idf|\n",
            "+---+--------+--------------------+\n",
            "|  2| Log4j -|(1590,[49,404,572...|\n",
            "|  3| Log4j -|(1590,[4,738],[2....|\n",
            "| 22|Apache h|(1590,[3,39,62,27...|\n",
            "| 27| Log4j -|(1590,[3,190,199]...|\n",
            "| 29| Log4j -|(1590,[3,59,92,11...|\n",
            "| 31| Log4j -|(1590,[44,49,123,...|\n",
            "| 32| Log4j -|(1590,[424],[5.76...|\n",
            "| 34| Log4j -|(1590,[8,95,112,6...|\n",
            "| 35| Log4j -|(1590,[121,127,47...|\n",
            "| 43| Log4j -|(1590,[0,193,395,...|\n",
            "| 44| Log4j -|(1590,[3,156],[2....|\n",
            "| 45| Batik -|(1590,[30,34,45,2...|\n",
            "| 46| Batik -|(1590,[30,34,618]...|\n",
            "| 47| Batik -|(1590,[292,618,88...|\n",
            "| 48| Batik -|(1590,[0,452,839,...|\n",
            "| 49| Batik -|(1590,[619,1124,1...|\n",
            "| 50| Batik -|(1590,[74,190,195...|\n",
            "| 51| Batik -|(1590,[35,212,541...|\n",
            "| 52| Batik -|(1590,[93,225,312...|\n",
            "| 53| Batik -|(1590,[116,219,61...|\n",
            "+---+--------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZZnoAzJ7k5n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_train = data.collect()\n",
        "# vectorizer = TfidfVectorizer()\n",
        "# tfidf = vectorizer.fit_transform(X_train)\n",
        "# # tfidf_cosine_similarities = linear_kernel(tfidf)\n",
        "# # shape_tfidf = tfidf_cosine_similarities.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-8BeNrd7k5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model-2: Similarity Score - TF-idf ----------------------------------------------------------------------------\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "tfidf_cosine_similarities = []\n",
        "execution_count_tfidf = 0\n",
        "\n",
        "def tfidf_preprocess_pyspark(id,df):\n",
        "    X_train = df['short_desc_processed']\n",
        "    print('TF-idf Vectorization and similarity score computation')\n",
        "    # Vectorization\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf = vectorizer.fit_transform(X_train)\n",
        "    # Calculate the cosine similarity score\n",
        "    tfidf_cosine_similarities = linear_kernel(tfidf)\n",
        "    print('TF-idf preprocess done')\n",
        "    \n",
        "    index_main   = df.loc[lambda df: df['id'] == id].index.array[0]\n",
        "    product_main = df.loc[lambda df: df['id'] == id,'product'].array[0]\n",
        "    tfidf_cosine_similarities_list = []\n",
        "    for index_other in range(shape_tfidf):\n",
        "        id_other      = df.iloc[index_other]['id']\n",
        "        product_other = df.iloc[index_other]['product']\n",
        "        if product_main == product_other:\n",
        "            tfidf_cosine_similarities_list.append([id_other,tfidf_cosine_similarities[index_main,index_other]])\n",
        "    #Conver to dataframe\n",
        "    tfidf_cosine_similarities_score_df = pd.DataFrame(tfidf_cosine_similarities_list, columns=['id','tfidf_score'])\n",
        "    tfidf_cosine_similarities_score_df = tfidf_cosine_similarities_score_df.reset_index(drop=True)\n",
        "    return tfidf_cosine_similarities_score_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r6eYcRw7k5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.functions import udf, struct, col\n",
        "from pyspark.sql.types import * \n",
        "import pyspark.sql.functions as func"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZw19YPf7k5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), False),\n",
        "    StructField(\"tfidf_score\", IntegerType(), False)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_so_ntQJ7k5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_preprocess_udf = udf(lambda z: tfidf_preprocess_pyspark(id, df), schema)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AV7QRxpt7k5x",
        "colab_type": "code",
        "colab": {},
        "outputId": "9067c96d-7dc9-4e0f-e908-a629a1116638"
      },
      "source": [
        "df_new = df.select('id', tfidf_preprocess_udf(2,processed_data_df))\n",
        "df_new.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/home/alireza/anaconda3/lib/python3.7/site-packages/pyspark/serializers.py\", line 590, in dumps\n",
            "    return cloudpickle.dumps(obj, 2)\n",
            "  File \"/home/alireza/anaconda3/lib/python3.7/site-packages/pyspark/cloudpickle.py\", line 863, in dumps\n",
            "    cp.dump(obj)\n",
            "  File \"/home/alireza/anaconda3/lib/python3.7/site-packages/pyspark/cloudpickle.py\", line 260, in dump\n",
            "    return Pickler.dump(self, obj)\n",
            "  File \"/home/alireza/anaconda3/lib/python3.7/pickle.py\", line 437, in dump\n",
            "    self.save(obj)\n",
            "  File \"/home/alireza/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/home/alireza/anaconda3/lib/python3.7/pickle.py\", line 774, in save_tuple\n",
            "    save(element)\n",
            "  File \"/home/alireza/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/home/alireza/anaconda3/lib/python3.7/site-packages/pyspark/cloudpickle.py\", line 400, in save_function\n",
            "    self.save_function_tuple(obj)\n",
            "  File \"/home/alireza/anaconda3/lib/python3.7/site-packages/pyspark/cloudpickle.py\", line 549, in save_function_tuple\n",
            "    save(state)\n",
            "  File \"/home/alireza/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/home/alireza/anaconda3/lib/python3.7/pickle.py\", line 859, in save_dict\n",
            "    self._batch_setitems(obj.items())\n",
            "  File \"/home/alireza/anaconda3/lib/python3.7/pickle.py\", line 885, in _batch_setitems\n",
            "    save(v)\n",
            "  File \"/home/alireza/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/home/alireza/anaconda3/lib/python3.7/pickle.py\", line 859, in save_dict\n",
            "    self._batch_setitems(obj.items())\n",
            "  File \"/home/alireza/anaconda3/lib/python3.7/pickle.py\", line 885, in _batch_setitems\n",
            "    save(v)\n",
            "  File \"/home/alireza/anaconda3/lib/python3.7/pickle.py\", line 549, in save\n",
            "    self.save_reduce(obj=obj, *rv)\n",
            "  File \"/home/alireza/anaconda3/lib/python3.7/pickle.py\", line 662, in save_reduce\n",
            "    save(state)\n",
            "  File \"/home/alireza/anaconda3/lib/python3.7/pickle.py\", line 504, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/home/alireza/anaconda3/lib/python3.7/pickle.py\", line 859, in save_dict\n",
            "    self._batch_setitems(obj.items())\n",
            "  File \"/home/alireza/anaconda3/lib/python3.7/pickle.py\", line 885, in _batch_setitems\n",
            "    save(v)\n",
            "  File \"/home/alireza/anaconda3/lib/python3.7/pickle.py\", line 524, in save\n",
            "    rv = reduce(self.proto)\n",
            "  File \"/home/alireza/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\", line 1257, in __call__\n",
            "    answer, self.gateway_client, self.target_id, self.name)\n",
            "  File \"/home/alireza/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\", line 63, in deco\n",
            "    return f(*a, **kw)\n",
            "  File \"/home/alireza/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\", line 332, in get_return_value\n",
            "    format(target_id, \".\", name, value))\n",
            "py4j.protocol.Py4JError: An error occurred while calling o352.__getstate__. Trace:\n",
            "py4j.Py4JException: Method __getstate__([]) does not exist\n",
            "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n",
            "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:274)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
            "\tat java.lang.Thread.run(Thread.java:748)\n",
            "\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "PicklingError",
          "evalue": "Could not serialize object: Py4JError: An error occurred while calling o352.__getstate__. Trace:\npy4j.Py4JException: Method __getstate__([]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcloudpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickleError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, protocol)\u001b[0m\n\u001b[1;32m    862\u001b[0m         \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m         \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_framing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTOP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_tuple\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m                 \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m             \u001b[0;31m# Subtle.  Same as in the big comment below.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function\u001b[0;34m(self, obj, name)\u001b[0m\n\u001b[1;32m    399\u001b[0m                 or themodule is None):\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_function_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function_tuple\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'qualname'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTUPLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    884\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    884\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# Save the reduce() output and finally memoize the object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, obj)\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m             \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m             \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUILD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    884\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m                 \u001b[0mrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    331\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m                     format(target_id, \".\", name, value))\n\u001b[0m\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o352.__getstate__. Trace:\npy4j.Py4JException: Method __getstate__([]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-120-a85b62ee44ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_preprocess_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprocessed_data_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/udf.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massigned\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0massignments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/udf.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mjudf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_judf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/udf.py\u001b[0m in \u001b[0;36m_judf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;31m# and should have a minimal performance impact.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_judf_placeholder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_judf_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_judf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_judf_placeholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/udf.py\u001b[0m in \u001b[0;36m_create_judf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mwrapped_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrap_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturnType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0mjdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseDataType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturnType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         judf = sc._jvm.org.apache.spark.sql.execution.python.UserDefinedPythonFunction(\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/udf.py\u001b[0m in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, returnType)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrap_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturnType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturnType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mpickled_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincludes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_for_python_RDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n\u001b[1;32m     37\u001b[0m                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;31m# the serialized command will be compressed by broadcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m     \u001b[0mser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0mpickled_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickled_command\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 1M\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m         \u001b[0;31m# The broadcast will have same life cycle as created PythonRDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    598\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Could not serialize object: %s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0mcloudpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: Py4JError: An error occurred while calling o352.__getstate__. Trace:\npy4j.Py4JException: Method __getstate__([]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxiN6Jwr7k5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGvKRKuL7k50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFV7773X7k51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZygBw1_P7k53",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8xg6BXi7k54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAKJtRhU7k56",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM1zGvjs7k57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrIl3Rd47k59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heqErpT37k6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lEZuacV7k6C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvWXbx1M7k6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCvfLkMI7k6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}