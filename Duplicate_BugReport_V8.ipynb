{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g4Y20uWiqC_o",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import progressbar as pb\n",
    "import time\n",
    "import warnings\n",
    "import text_processing as tp\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wcgC4k6LqC_t",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Import dataset\n",
    "# data = pd.read_csv('https://zenodo.org/record/400614/files/apache.csv?download=1',sep=',')\n",
    "# # Remove nan from the main dataset\n",
    "# remove_nan(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YSGUykkOqC_w",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# # Preprocessing the short_desc\n",
    "# processed_data_df = tp.text_preprocessing(data,\"short_desc\")\n",
    "# # Save to file to save time\n",
    "# processed_data_df.to_csv('~/Desktop/Google-Drive/Colab Notebooks/processed_data_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BerhLMLBqC_z",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Load prosecced df from file to sasve time\n",
    "processed_data_df = pd.read_csv('~/Desktop/Google-Drive/Colab Notebooks/processed_data_df.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KlpjDsOiqC_1",
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model-1: Similarity Score - Word2vec -------------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import time\n",
    "import progressbar as pb\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "execution_count_word2vec = 0\n",
    "processed_data_nlp_df = []\n",
    "\n",
    "# Convert short_desc str to nlp format to fasten the computation process \n",
    "def word2vec_preprocess(df):\n",
    "    print('Convert short_desc str to nlp format')\n",
    "    sample_size = len(df)\n",
    "    progress = pb.ProgressBar(maxval = sample_size).start()\n",
    "    progvar  = 0\n",
    "    processed_data_nlp = []\n",
    "    for tup in df.itertuples():\n",
    "        processed_data_nlp.append((tup.id,tup.product,nlp(tup.short_desc_processed))) \n",
    "        progress.update(progvar + 1)\n",
    "        progvar += 1\n",
    "    global processed_data_nlp_df\n",
    "    processed_data_nlp_df = pd.DataFrame(processed_data_nlp, columns=['id','product','short_desc_processed'])\n",
    "    global execution_count_word2vec\n",
    "    execution_count_word2vec += 1\n",
    "\n",
    "# Calculate the cosine similarity score\n",
    "def word2vec_similarity(id,df):\n",
    "    if execution_count_word2vec == 0:\n",
    "        word2vec_preprocess(df)\n",
    "    similarities_score_list = []\n",
    "    product_main = processed_data_nlp_df.loc[lambda df: df['id'] == id,'product'].array[0]\n",
    "    short_desc_processed_main = processed_data_nlp_df.loc[lambda df: df['id'] == id,'short_desc_processed'].array[0]\n",
    "    for doc in processed_data_nlp_df.itertuples():\n",
    "        product_other = processed_data_nlp_df.loc[lambda df: df['id'] == doc.id,'product'].array[0]\n",
    "        if product_main == product_other:\n",
    "            similarity_score = doc.short_desc_processed.similarity(short_desc_processed_main)\n",
    "            similarities_score_list.append((doc.id,similarity_score))\n",
    "    #convert to dataframe\n",
    "    word2vec_similarities_score_df = pd.DataFrame(similarities_score_list, columns=['id','word2vec_score'] )\n",
    "    word2vec_similarities_score_df = word2vec_similarities_score_df.reset_index(drop=True)\n",
    "    return word2vec_similarities_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-gU1hrZ1qC_3",
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model-2: Similarity Score - TF-idf ----------------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import time\n",
    "import progressbar as pb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "tfidf_cosine_similarities = []\n",
    "execution_count_tfidf = 0\n",
    "\n",
    "def tfidf_preprocess(df):\n",
    "    X_train = df['short_desc_processed']\n",
    "    print('TF-idf Vectorization and similarity score computation')\n",
    "    # Vectorization\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf = vectorizer.fit_transform(X_train)\n",
    "    # Calculate the cosine similarity score\n",
    "    global tfidf_cosine_similarities\n",
    "    tfidf_cosine_similarities = linear_kernel(tfidf)\n",
    "    global shape_tfidf\n",
    "    shape_tfidf = tfidf_cosine_similarities.shape[0]\n",
    "    print('TF-idf preprocess done')\n",
    "    global execution_count_tfidf\n",
    "    execution_count_tfidf += 1\n",
    "\n",
    "def tfidf_similarities(id,df):\n",
    "    if execution_count_tfidf == 0:\n",
    "        tfidf_preprocess(df)\n",
    "    index_main   = df.loc[lambda df: df['id'] == id].index.array[0]\n",
    "    product_main = df.loc[lambda df: df['id'] == id,'product'].array[0]\n",
    "    tfidf_cosine_similarities_list = []\n",
    "    for index_other in range(shape_tfidf):\n",
    "        id_other      = df.iloc[index_other]['id']\n",
    "        product_other = df.iloc[index_other]['product']\n",
    "        if product_main == product_other:\n",
    "            tfidf_cosine_similarities_list.append([id_other,tfidf_cosine_similarities[index_main,index_other]])\n",
    "    #Conver to dataframe\n",
    "    tfidf_cosine_similarities_score_df = pd.DataFrame(tfidf_cosine_similarities_list, columns=['id','tfidf_score'])\n",
    "    tfidf_cosine_similarities_score_df = tfidf_cosine_similarities_score_df.reset_index(drop=True)\n",
    "    return tfidf_cosine_similarities_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z5wlDGp3qC_6",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Model-3: Similarity Score - BM24F -----------------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import time\n",
    "import progressbar as pb\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "processed_corpus_list = []\n",
    "bm25 = []\n",
    "execution_count_bm25 = 0\n",
    "\n",
    "# preprocess - tokenize the short_desc to token\n",
    "def bm24_preprocess(df):\n",
    "    print('preprocess - tokenize the short_desc to token')\n",
    "    sample_size = len(df)\n",
    "    global processed_corpus_list\n",
    "    processed_corpus_list = []\n",
    "    for x in df.itertuples():\n",
    "        short_desc_splited = x.short_desc_processed.split(\" \")\n",
    "        processed_corpus_list.append(short_desc_splited)\n",
    "    # Create a MB24 Object with the corpus\n",
    "    global bm25\n",
    "    bm25 = BM25Okapi(processed_corpus_list)\n",
    "    global execution_count_bm25\n",
    "    execution_count_bm25 += 1\n",
    "    \n",
    "# Calculate the similarity score\n",
    "def bm24_similarity(id,df):\n",
    "    if execution_count_bm25 == 0:\n",
    "        bm24_preprocess(df)\n",
    "    index_main    = df.loc[lambda df: df['id'] == id].index.array[0]\n",
    "    product_main  = df.loc[lambda df: df['id'] == id,'product'].array[0]\n",
    "    query         = processed_corpus_list[index_main]\n",
    "    doc_scores    = bm25.get_scores(query)\n",
    "    doc_scores_df = pd.DataFrame(doc_scores, columns=['bm24_score'])\n",
    "    # add id to the score list and remove unsimiliar product\n",
    "    blanks = []\n",
    "    for x in doc_scores_df.itertuples():\n",
    "        id_other      = df.iloc[x.Index]['id']\n",
    "        product_other = df.iloc[x.Index]['product']\n",
    "        # add id to the score list\n",
    "        doc_scores_df.loc[x.Index,'id'] = id_other\n",
    "        if product_main != product_other:\n",
    "            blanks.append(x.Index)    \n",
    "    doc_scores_df.drop(blanks,inplace=True)   \n",
    "    doc_scores_df = doc_scores_df.reset_index(drop = True)  \n",
    "    return doc_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1z8EslEyqC_8"
   },
   "source": [
    "Assessment the Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ikHJjrkvqC_9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Assessment the Model Accuracy -------------------------------------------------------------------------------\n",
    "# duplicate_df = pd.read_csv('https://zenodo.org/record/400614/files/apache.relations.csv?download=1',sep=',')\n",
    "duplicate_df = pd.read_csv('~/Desktop/Google-Drive/Colab Notebooks/duplicate_df.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_qFg56-HqC__",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the similarity scores and return the first n top scoes\n",
    "def similarity_score(id, df,top_n):\n",
    "    word2vec_similarity_df = word2vec_similarity(id, df).sort_values(by=['word2vec_score'],ascending=False).head(top_n)\n",
    "    tfidf_similarity_df    = tfidf_similarities(id, df).sort_values(by=['tfidf_score'],ascending=False).head(top_n)\n",
    "    bm24_similarity_df     = bm24_similarity(id, df).sort_values(by=['bm24_score'],ascending=False).head(top_n)    \n",
    "    return word2vec_similarity_df, tfidf_similarity_df, bm24_similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lgayCI1DqDAC",
    "outputId": "f01c78d2-a083-44ac-f740-744535e96df2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "N/A% (0 of 500) |                        | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Total- 0.009871721267700195 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Calculate the similarity score for the reported duplicated bug report\n",
    "import progressbar as pb\n",
    "import time\n",
    "start_time  = time.time()\n",
    "sample_size = len(duplicate_df)\n",
    "progress    = pb.ProgressBar(maxval = sample_size).start()\n",
    "progvar     = 0\n",
    "duplicated_similarity_score_list = []\n",
    "\n",
    "for tup in duplicate_df.itertuples():\n",
    "#     word2vec_similarity_df, tfidf_similarity_df, bm24_similarity_df = similarity_score(tup.id, processed_data_df,20)\n",
    "#     duplicated_similarity_score_list.append([tup.id,tup.dup,word2vec_similarity_df,tfidf_similarity_df,bm24_similarity_df])\n",
    "    progress.update(progvar + 1)\n",
    "    progvar += 1\n",
    "    \n",
    "print(\"-Total- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4DtDQ8rhqDAF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save the list to a file since it takes 8 hours to create it\n",
    "with open(\"duplicated_similarity_score_list.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(duplicated_similarity_score_list, fp)\n",
    "\n",
    "# Read the list from a file \n",
    "with open(\"duplicated_similarity_score_list.txt\", \"rb\") as fp:   # Unpickling\n",
    "    duplicated_similarity_score_list = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JKSY0w_cqJXg"
   },
   "outputs": [],
   "source": [
    "# Calculate the numbe of found duplicated report in the a algorithem\n",
    "def result(name_of_algorthem):\n",
    "    if name_of_algorthem == 'word2vec':\n",
    "        select_algorithm = 2\n",
    "        print('word2vec result:')\n",
    "    elif name_of_algorthem == 'tfidf':\n",
    "        select_algorithm = 3\n",
    "        print('TF-idf result:')\n",
    "    elif name_of_algorthem == 'bm24':\n",
    "        select_algorithm = 4\n",
    "        print('bm24 result:')\n",
    "    else:\n",
    "        return \"Wrong selection\"\n",
    "    found_counter = 0\n",
    "    not_found_counter = 0\n",
    "    for x in duplicated_similarity_score_list:\n",
    "        dup = x[1]\n",
    "        df = x[select_algorithm]\n",
    "        if df.loc[df['id'] == dup].empty:\n",
    "            not_found_counter +=1\n",
    "        else:\n",
    "            found_counter +=1\n",
    "    print('Num of duplicated report found: ',found_counter)\n",
    "    print('Num of duplicated report not found: ',not_found_counter)\n",
    "    print('Recall (TP/TP+FN): ', found_counter/500*100,'%\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d8uPULPPqa4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec result:\n",
      "Num of duplicated report found:  150\n",
      "Num of duplicated report not found:  350\n",
      "Recall (TP/TP+FN):  30.0 %\n",
      "\n",
      "TF-idf result:\n",
      "Num of duplicated report found:  178\n",
      "Num of duplicated report not found:  322\n",
      "Recall (TP/TP+FN):  35.6 %\n",
      "\n",
      "bm24 result:\n",
      "Num of duplicated report found:  174\n",
      "Num of duplicated report not found:  326\n",
      "Recall (TP/TP+FN):  34.8 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the results\n",
    "result('word2vec')\n",
    "result('tfidf')\n",
    "result('bm24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ycZoP4yBqOCj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Duplicate_BugReport_V7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
