{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2OHHmqSI6H62"
   },
   "outputs": [],
   "source": [
    "# Model-1-V4\n",
    "# - Load data set\n",
    "# - Remove NaN in short_desc\n",
    "# - Convert to lowercase\n",
    "# - Split the words using 1. ASCII character identification for english 2. Split by Space  3. wordninja\n",
    "# - Apply normlise\n",
    "# - Apply contractions/expansions \n",
    "# - Remove punctuations\n",
    "# - remove tags.\n",
    "# - remove special characters and digits.\n",
    "# - Stemming/Lemmatisation. \n",
    "# - Prepare the whole data set\n",
    "# - Calculate the similarity between every two bugreport for the same product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SE3nFMwg6XEB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2DH0GHXC6cQv"
   },
   "outputs": [],
   "source": [
    "data_org = pd.read_csv('https://zenodo.org/record/400614/files/apache.csv?download=1',sep=',')\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem in reading csv file, cut the short_desc after 88th character\n",
    "# print(data.iloc[[124]]['short_desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 641,
     "status": "ok",
     "timestamp": 1576509940564,
     "user": {
      "displayName": "Alireza Ghasemieh",
      "photoUrl": "https://lh5.googleusercontent.com/--sm1QCsEBLM/AAAAAAAAAAI/AAAAAAAABII/jRVLOB4QFgQ/s64/photo.jpg",
      "userId": "15022417890264208659"
     },
     "user_tz": 300
    },
    "id": "B_bfdghVNWRk",
    "outputId": "ff167a96-c57e-435b-87d4-dd1df9e57bab"
   },
   "outputs": [],
   "source": [
    "data = data_org[['id','product','short_desc']]\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2810,
     "status": "ok",
     "timestamp": 1576509943167,
     "user": {
      "displayName": "Alireza Ghasemieh",
      "photoUrl": "https://lh5.googleusercontent.com/--sm1QCsEBLM/AAAAAAAAAAI/AAAAAAAABII/jRVLOB4QFgQ/s64/photo.jpg",
      "userId": "15022417890264208659"
     },
     "user_tz": 300
    },
    "id": "Hdo7ksWKBANr",
    "outputId": "6fb03693-9307-430a-b55e-7adb6f21ae1c"
   },
   "outputs": [],
   "source": [
    "def remove_nan(df):\n",
    "    blanks = []\n",
    "    print(\"Before removing the NaN:\")\n",
    "    print(df.isnull().sum())\n",
    "    for x in df.itertuples():\n",
    "        if type(x.short_desc)!=str: # detect the NaN\n",
    "            blanks.append(x.Index)\n",
    "        elif not x.short_desc: # detect empty string\n",
    "            blanks.append(x.Index)\n",
    "    df.drop(blanks,inplace=True)\n",
    "    print(\"\\nAfter removing the NaN:\\n\",df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing the NaN:\n",
      "id             0\n",
      "product        0\n",
      "short_desc    58\n",
      "dtype: int64\n",
      "\n",
      "After removing the NaN:\n",
      " id            0\n",
      "product       0\n",
      "short_desc    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "remove_nan(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6212,
     "status": "ok",
     "timestamp": 1576510168345,
     "user": {
      "displayName": "Alireza Ghasemieh",
      "photoUrl": "https://lh5.googleusercontent.com/--sm1QCsEBLM/AAAAAAAAAAI/AAAAAAAABII/jRVLOB4QFgQ/s64/photo.jpg",
      "userId": "15022417890264208659"
     },
     "user_tz": 300
    },
    "id": "0vbHfZFrs_Ab",
    "outputId": "ce6f19dc-84e6-4ef9-aa72-234334a3f70f"
   },
   "outputs": [],
   "source": [
    "# conda install -c conda-forge spacy\n",
    "# !pip install wordninja\n",
    "# !pip install normalise\n",
    "# !pip install pycontractions\n",
    "# conda install -c conda-forge spacy-lookups-data\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JGomxQMNLrp9"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "import nltk\n",
    "# nltk.download('brown')\n",
    "# nltk.download('names')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('universal_tagset')\n",
    "import wordninja as nj #for spliting the words in each documents\n",
    "import normalise as ns \n",
    "from pycontractions import Contractions # for expansion and contrations\n",
    "import contractions\n",
    "import re # remove tags.\n",
    "\n",
    "def preprocessing(df,id):\n",
    "    # Extract and convert short_desc to string\n",
    "    row = df[df['id'] == int(id)]                                # Extract a tuple from the dataframe\n",
    "    short_desc_to_string = row.short_desc.to_string(index=False) # Conver short_desc to string\n",
    "    short_desc_to_string = short_desc_to_string[1:]              # Remove the first space char from the begnning\n",
    "\n",
    "#     my_short_desc_to_string = \"I'd liked to said <pclass='test'> CANADAisn't Alireza@gmail.com the worst-countries in the 44 world!!!\"\n",
    "#     print(\"Original:\\n\" + my_short_desc_to_string)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    short_desc_lowercase = short_desc_to_string.lower()\n",
    "#     print(\"\\nshort_desc_lowercase:\\n\" + short_desc_lowercase)\n",
    "    \n",
    "    # Split the words using 1. ASCII character identification for english 2. Split by Space  3. wordninja\n",
    "    short_desc_splited = nj.split(short_desc_lowercase)\n",
    "#     print(\"\\nshort_desc_splited:\\n\" + str(short_desc_splited))\n",
    "    \n",
    "    # Apply normlise\n",
    "    short_desc_normalised = ns.normalise(short_desc_splited,verbose=False)\n",
    "    short_desc_normalised_listToStr = ' '.join(map(str, short_desc_normalised))\n",
    "    \n",
    "#     print(\"\\nshort_desc_normalised:\\n\" + short_desc_normalised_listToStr)\n",
    "    \n",
    "    # Apply contractions/expansions \n",
    "#     cont = Contractions(api_key=\"glove-twitter-100\") #We shold use this one but it shows error \n",
    "    short_desc_contract = contractions.fix(short_desc_normalised_listToStr) \n",
    "#     print(\"\\nshort_desc_contract:\\n\" + str(short_desc_contract))\n",
    "        \n",
    "    # remove tags.\n",
    "    clean = re.compile('<.*?>')\n",
    "    short_desc_removed_tag = re.sub(clean, '', str(short_desc_contract)) \n",
    "#     print(\"\\nshort_desc_removed_tag:\\n\" + str(short_desc_removed_tag))\n",
    "    \n",
    "    # remove special characters and digits.\n",
    "    short_desc_removed_special_char = [re.sub(r\"[^a-zA-Z]+\", ' ', k) for k in str(short_desc_removed_tag).split(\"\\n\")]\n",
    "#     print(\"\\nshort_desc_removed_special_char:\\n\" + str(short_desc_removed_special_char))\n",
    "\n",
    "    # Remove Punctuations and Stop words\n",
    "    short_desc_doc = nlp(str(short_desc_removed_special_char))  \n",
    "    short_desc_map = map(lambda token: token if (token.is_punct == False and token.is_stop == False) else None, short_desc_doc)\n",
    "    short_desc_list = list(short_desc_map)        \n",
    "    \n",
    "    # Convert list to string and remove one-character word\n",
    "    short_desc_string = \"\"     \n",
    "    for element in short_desc_list:\n",
    "        if element != None and len(element)>1:\n",
    "            short_desc_string += str(element) +' '\n",
    "    short_desc_string = short_desc_string[:-1]\n",
    "#     print(\"\\nshort_desc_string-1:\\n\" + short_desc_string)\n",
    "\n",
    "#     short_desc_string2 = ' '.join(map(lambda x: str(x) if (x != None and len(x)>1) else None, short_desc_list))\n",
    "#     print(\"\\nshort_desc_string-2:\\n\" + short_desc_string2)\n",
    "            \n",
    "    # Stemming/Lemmatisation.       \n",
    "    short_desc_lemmata = [token.lemma_ for token in nlp(short_desc_string)]\n",
    "#     print(\"short_desc_lemmata:\\n\" + str(short_desc_lemmata))\n",
    "    short_desc_preprocessed = ' '.join(map(str, short_desc_lemmata))\n",
    "        \n",
    "    return short_desc_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CsB9wctvXbxm",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80% (4 of 5) |####################      | Elapsed Time: 0:00:00 ETA:   0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.3636445999145508 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product</th>\n",
       "      <th>short_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Log4j -</td>\n",
       "      <td>testing book zee la setup log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Log4j -</td>\n",
       "      <td>file share</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>Apache h</td>\n",
       "      <td>need hook user define error handle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>Log4j -</td>\n",
       "      <td>error forget layout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>Log4j -</td>\n",
       "      <td>call category error object throw able thro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   product                                  short_desc\n",
       "0   2   Log4j -               testing book zee la setup log\n",
       "1   3   Log4j -                                  file share\n",
       "2  22  Apache h          need hook user define error handle\n",
       "3  27   Log4j -                         error forget layout\n",
       "4  29   Log4j -  call category error object throw able thro"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the vectors for each document\n",
    "import time\n",
    "import progressbar as pb\n",
    "start_time = time.time()\n",
    "\n",
    "sample_size = 5 # Sample Size\n",
    "progress = pb.ProgressBar(maxval = sample_size).start()\n",
    "\n",
    "# dataset_length = len(data)\n",
    "# progress = pb.ProgressBar(maxval = dataset_length).start()\n",
    "\n",
    "progvar = 0\n",
    "processed_data_list = []\n",
    "\n",
    "for x in data.itertuples():\n",
    "    string = preprocessing(data,x.id)\n",
    "    processed_data_list.append((x.id,x.product,string))   \n",
    "    \n",
    "    # Show the progress in the output\n",
    "    progress.update(progvar+1)  \n",
    "    progvar += 1    \n",
    "    \n",
    "    # Terminate the process when reach to sample size\n",
    "    if progvar >= sample_size:\n",
    "        break\n",
    "# Convert list to dataframe\n",
    "processed_data_df = pd.DataFrame(processed_data_list, columns=['id','product','short_desc'])\n",
    "# del data\n",
    "del processed_data_list\n",
    "# show the time of process        \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "processed_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_df.to_csv('~/Desktop/Google-Drive/Colab Notebooks/processed_data_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_id(df):\n",
    "    length = len(df)\n",
    "    for tup in df.itertuples():\n",
    "        id1 = data_org.iloc[tup.index1]['id']\n",
    "        id2 = data_org.iloc[tup.index2]['id']\n",
    "        df.loc[tup.Index,'index1'] = id1\n",
    "        df.loc[tup.Index,'index2'] = id2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model-1: Similarity Score - Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "It9SV878P1JL",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "N/A% (0 of 5) |                          | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.09471583366394043 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index1</th>\n",
       "      <th>index2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>29</td>\n",
       "      <td>0.728128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>0.728128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>0.582265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>0.582265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>0.551018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index1  index2     score\n",
       "0      27      29  0.728128\n",
       "1      29      27  0.728128\n",
       "2       2      27  0.582265\n",
       "3      27       2  0.582265\n",
       "4       3      27  0.551018"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate similarity score Using word2vec\n",
    "start_time = time.time()\n",
    "dataset_length = len(processed_data_df)\n",
    "progress = pb.ProgressBar(maxval = dataset_length).start()\n",
    "progvar = 0\n",
    "\n",
    "# Convert short_desc str to nlp format to fasten the computation process\n",
    "processed_data_nlp = []\n",
    "\n",
    "for tup in processed_data_df.itertuples():\n",
    "    processed_data_nlp.append((tup.id,tup.product,nlp(tup.short_desc)))\n",
    "    progress.update(progvar+1)  \n",
    "    progvar += 1\n",
    "    \n",
    "processed_data_nlp_df = pd.DataFrame(processed_data_nlp, columns=['id','product','short_desc'])\n",
    "processed_data_nlp_df.head()\n",
    "del processed_data_nlp\n",
    "\n",
    "# Calculate the similarity score\n",
    "progress = pb.ProgressBar(maxval = dataset_length).start()\n",
    "\n",
    "similarities_score_list = []\n",
    "\n",
    "for doc1 in processed_data_nlp_df.itertuples():\n",
    "    for doc2 in processed_data_nlp_df.itertuples():\n",
    "        if (doc1.id != doc2.id) and (doc1.product == doc2.product): # if two bug reports belong to the same product then check the similarity\n",
    "            similarity_score = doc1.short_desc.similarity(doc2.short_desc)\n",
    "            similarities_score_list.append((doc1.id,doc2.id,similarity_score))         \n",
    "    # Show the progress in the output        \n",
    "    progress.update(doc1.Index+1)  \n",
    "    \n",
    "\n",
    "# Sort based on the score and \n",
    "sorted_similarities_score = sorted(similarities_score_list, key=lambda tup: tup[2], reverse=True)\n",
    "del similarities_score_list\n",
    "\n",
    "#convert to dataframe\n",
    "word2vec_similarities_score_df = pd.DataFrame(sorted_similarities_score, columns=['index1','index2','score'] )\n",
    "del sorted_similarities_score\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time)) # show the time of process\n",
    "word2vec_similarities_score_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_data_df.to_csv('~/Desktop/Google-Drive/Colab Notebooks/word2vec_similarities_score_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jw08fd9KAFuA"
   },
   "source": [
    "Model-2: Similarity Score - tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "N/A% (0 of 5) |                          | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0071184635162353516 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index1</th>\n",
       "      <th>index2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.122796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.112874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.075667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index1  index2     score\n",
       "0       2       3  0.122796\n",
       "1       3       4  0.112874\n",
       "2       2       4  0.075667\n",
       "3       0       1  0.000000\n",
       "4       0       2  0.000000"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "X_train = processed_data_df['short_desc']\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Vectorization \n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(X_train) # remember to use the original X_train set\n",
    "\n",
    "# Similarity score calculation\n",
    "tfidf_cosine_similarities = linear_kernel(tfidf)\n",
    "\n",
    "shape = tfidf_cosine_similarities.shape\n",
    "progress = pb.ProgressBar(maxval = shape[0]).start()\n",
    "\n",
    "tfidf_cosine_similarities_list = []\n",
    "\n",
    "for index1 in range(shape[0]):\n",
    "    for index2 in range(shape[1]):\n",
    "        if index2 > index1:\n",
    "            tfidf_cosine_similarities_list.append([index1,index2,tfidf_cosine_similarities[index1,index2]])\n",
    "            \n",
    "    progress.update(index1 + 1)  \n",
    "    \n",
    "# Sort the score list\n",
    "tfidf_cosine_similarities_sort = sorted(tfidf_cosine_similarities_list, key=lambda tup: tup[2], reverse=True)\n",
    "del tfidf_cosine_similarities_list\n",
    "\n",
    "#Conver to dataframe\n",
    "tfidf_cosine_similarities_score_df = pd.DataFrame(tfidf_cosine_similarities_sort, columns=['index1','index2','score'])\n",
    "del tfidf_cosine_similarities_sort\n",
    "\n",
    "# Convert index to id\n",
    "index_to_id(tfidf_cosine_similarities_score_df)\n",
    "tfidf_cosine_similarities_score_df.rename(columns={\"index1\": \"id1\", \"index2\": \"id2\"})\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time)) # show the time of process\n",
    "tfidf_cosine_similarities_score_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_data_df.to_csv('~/Desktop/Google-Drive/Colab Notebooks/tfidf_cosine_similarities_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model-3: Similarity Score - BM24F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "N/A% (0 of 5) |                          | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.007170915603637695 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index1</th>\n",
       "      <th>index2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.310791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.214172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.214172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index1  index2     score\n",
       "0       4       5  0.310791\n",
       "1       4       6  0.214172\n",
       "2       5       6  0.214172\n",
       "3       2       3  0.000000\n",
       "4       2       4  0.000000"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "start_time = time.time()\n",
    "progress = pb.ProgressBar(maxval = len(processed_data_df)).start()\n",
    "\n",
    "# preprocess - tokenize the short_desc to token\n",
    "processed_corpus_list = []\n",
    "for x in processed_data_df.itertuples():\n",
    "    short_desc_splited = x.short_desc.split(\" \")\n",
    "    processed_corpus_list.append(short_desc_splited)\n",
    "    \n",
    "# Create a MB24 Object with the corpus\n",
    "bm25 = BM25Okapi(processed_corpus_list)\n",
    "\n",
    "# Calculat the similarity score for all bug reports\n",
    "BM24_similarity_score_list = []\n",
    "\n",
    "for x in processed_data_df.itertuples():\n",
    "    query = processed_corpus_list[x.Index]\n",
    "    doc_scores = bm25.get_scores(query)\n",
    "    BM24_similarity_score_list.append([x.Index,doc_scores])\n",
    "\n",
    "# Reformating \n",
    "shape = doc_scores.shape[0]\n",
    "BM24_similarity_score_list_2 = []\n",
    "\n",
    "for index1 in range(shape):\n",
    "    for index2 in range(shape):\n",
    "        if index2 > index1:\n",
    "            BM24_similarity_score_list_2.append([index1+2,index2+2,BM24_similarity_score_list[index1][1][index2]])  \n",
    "    \n",
    "    progress.update(index1 + 1)  \n",
    "            \n",
    "del BM24_similarity_score_list\n",
    "\n",
    "# Sort the score list\n",
    "BM24_similarity_score_sort = sorted(BM24_similarity_score_list_2, key=lambda tup: tup[2], reverse=True)\n",
    "del BM24_similarity_score_list_2\n",
    "\n",
    "#Conver to dataframe\n",
    "BM24_similarity_score_sort_df = pd.DataFrame(BM24_similarity_score_sort, columns=['index1','index2','score'])\n",
    "del BM24_similarity_score_sort\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time)) # show the time of process\n",
    "\n",
    "# Convert index to id\n",
    "index_to_id(BM24_similarity_score_sort_df)\n",
    "BM24_similarity_score_sort_df.rename(columns={\"index1\": \"id1\", \"index2\": \"id2\"})\n",
    "\n",
    "BM24_similarity_score_sort_df.head()\n",
    "    \n",
    "# bm25.get_top_n(tokenized_query, processed_corpus_list, n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessment the Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicat_org = pd.read_csv('https://zenodo.org/record/400614/files/apache.relations.csv?download=1',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Model-1_V04.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
