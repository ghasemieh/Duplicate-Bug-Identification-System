{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model-1_V05.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ghasemieh/Duplicate-Bug-Identification-System/blob/master/Model_1_V05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2OHHmqSI6H62",
        "colab": {}
      },
      "source": [
        "# Model-1-V5\n",
        "# - Load data set\n",
        "# - Remove NaN in short_desc\n",
        "# - Convert to lowercase\n",
        "# - Split the words using 1. ASCII character identification for english 2. Split by Space  3. wordninja\n",
        "# - Apply normlise\n",
        "# - Apply contractions/expansions \n",
        "# - Remove punctuations\n",
        "# - remove tags.\n",
        "# - remove special characters and digits.\n",
        "# - Stemming/Lemmatisation. \n",
        "# - Prepare the whole data set\n",
        "# - Calculate the similarity between every two bugreport for the same product"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SE3nFMwg6XEB",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2DH0GHXC6cQv",
        "colab": {}
      },
      "source": [
        "data_org = pd.read_csv('https://zenodo.org/record/400614/files/apache.csv?download=1',sep=',')\n",
        "data = data_org[['id','product','short_desc']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hdo7ksWKBANr",
        "colab": {}
      },
      "source": [
        "def remove_nan(df):\n",
        "    blanks = []\n",
        "    print(\"Before removing the NaN:\")\n",
        "    print(df.isnull().sum())\n",
        "    for x in df.itertuples():\n",
        "        if type(x.short_desc)!=str: # detect the NaN\n",
        "            blanks.append(x.Index)\n",
        "        elif not x.short_desc: # detect empty string\n",
        "            blanks.append(x.Index)\n",
        "    df.drop(blanks,inplace=True)\n",
        "    print(\"\\nAfter removing the NaN:\\n\",df.isnull().sum())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UzlErImuJhG",
        "colab_type": "code",
        "colab": {},
        "outputId": "68063a6a-51f3-4bf3-971e-084c6b17536d"
      },
      "source": [
        "remove_nan(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before removing the NaN:\n",
            "id             0\n",
            "product        0\n",
            "short_desc    58\n",
            "dtype: int64\n",
            "\n",
            "After removing the NaN:\n",
            " id            0\n",
            "product       0\n",
            "short_desc    0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0vbHfZFrs_Ab",
        "colab": {}
      },
      "source": [
        "# conda install -c conda-forge spacy\n",
        "# !pip install wordninja\n",
        "# !pip install normalise\n",
        "# !pip install pycontractions\n",
        "# conda install -c conda-forge spacy-lookups-data\n",
        "# !python -m spacy download en_core_web_lg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JGomxQMNLrp9",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "import nltk\n",
        "# nltk.download('brown')\n",
        "# nltk.download('names')\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "# nltk.download('universal_tagset')\n",
        "import wordninja as nj #for spliting the words in each documents\n",
        "import normalise as ns \n",
        "from pycontractions import Contractions # for expansion and contrations\n",
        "import contractions\n",
        "import re # remove tags.\n",
        "\n",
        "def preprocessing(df,id):\n",
        "    # Extract and convert short_desc to string\n",
        "    row = df[df['id'] == int(id)]                                # Extract a tuple from the dataframe\n",
        "    short_desc_to_string = row.short_desc.to_string(index=False) # Conver short_desc to string\n",
        "    short_desc_to_string = short_desc_to_string[1:]              # Remove the first space char from the begnning\n",
        "    \n",
        "    # Convert to lowercase\n",
        "    short_desc_lowercase = short_desc_to_string.lower()\n",
        "    \n",
        "    # Split the words using 1. ASCII character identification for english 2. Split by Space  3. wordninja\n",
        "    short_desc_splited = nj.split(short_desc_lowercase)\n",
        "    \n",
        "    # Apply normlise\n",
        "    short_desc_normalised = ns.normalise(short_desc_splited,verbose=False)\n",
        "    short_desc_normalised_listToStr = ' '.join(map(str, short_desc_normalised))\n",
        "    \n",
        "    # Apply contractions/expansions \n",
        "    short_desc_contract = contractions.fix(short_desc_normalised_listToStr) \n",
        "        \n",
        "    # remove tags.\n",
        "    clean = re.compile('<.*?>')\n",
        "    short_desc_removed_tag = re.sub(clean, '', str(short_desc_contract)) \n",
        "    \n",
        "    # remove special characters and digits.\n",
        "    short_desc_removed_special_char = [re.sub(r\"[^a-zA-Z]+\", ' ', k) for k in str(short_desc_removed_tag).split(\"\\n\")]\n",
        "\n",
        "    # Remove Punctuations and Stop words\n",
        "    short_desc_doc = nlp(str(short_desc_removed_special_char))  \n",
        "    short_desc_map = map(lambda token: token if (token.is_punct == False and token.is_stop == False) else None, short_desc_doc)\n",
        "    short_desc_list = list(short_desc_map)        \n",
        "    \n",
        "    # Convert list to string and remove one-character word\n",
        "    short_desc_string = \"\"     \n",
        "    for element in short_desc_list:\n",
        "        if element != None and len(element)>1:\n",
        "            short_desc_string += str(element) +' '\n",
        "    short_desc_string = short_desc_string[:-1]\n",
        "            \n",
        "    # Stemming/Lemmatisation.       \n",
        "    short_desc_lemmata = [token.lemma_ for token in nlp(short_desc_string)]\n",
        "    short_desc_preprocessed = ' '.join(map(str, short_desc_lemmata))\n",
        "        \n",
        "    return short_desc_preprocessed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CsB9wctvXbxm",
        "scrolled": true,
        "colab": {},
        "outputId": "91a5bc49-b4e7-42eb-a867-30bd09b5324a"
      },
      "source": [
        "# Prepare the vectors for each document\n",
        "import time\n",
        "import progressbar as pb\n",
        "start_time = time.time()\n",
        "\n",
        "# sample_size = 100 # Sample Size\n",
        "# progress = pb.ProgressBar(maxval = sample_size).start()\n",
        "\n",
        "dataset_length = len(data)\n",
        "progress = pb.ProgressBar(maxval = dataset_length).start()\n",
        "\n",
        "progvar = 0\n",
        "processed_data_list = []\n",
        "\n",
        "for x in data.itertuples():\n",
        "    string = preprocessing(data,x.id)\n",
        "    processed_data_list.append((x.id,x.product,string))   \n",
        "    \n",
        "    # Show the progress in the output\n",
        "    progress.update(progvar+1)  \n",
        "    progvar += 1    \n",
        "    \n",
        "    # Terminate the process when reach to sample size\n",
        "#     if progvar >= sample_size:\n",
        "#         break\n",
        "# Convert list to dataframe\n",
        "processed_data_df = pd.DataFrame(processed_data_list, columns=['id','product','short_desc'])\n",
        "del processed_data_list\n",
        "\n",
        "# show the time of process        \n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "processed_data_df.to_csv('~/Desktop/Google-Drive/Colab Notebooks/processed_data_df.csv',index=False)\n",
        "processed_data_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  5% (2212 of 43991) |                   | Elapsed Time: 0:02:38 ETA:   0:40:10"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "re4a9CBtuJhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def index_to_id(df):\n",
        "    length = len(df)\n",
        "    for tup in df.itertuples():\n",
        "        df.loc[tup.Index,'id1'] = data_org.iloc[tup.id1]['id']\n",
        "        df.loc[tup.Index,'id2'] = data_org.iloc[tup.id2]['id']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34krTvsYuJhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_diff_product_score(df):\n",
        "    for tup in df.itertuples():\n",
        "        product1 = data_org.loc[lambda df: df['id'] == tup.id1,'product'].array[0]\n",
        "        product2 = data_org.loc[lambda df: df['id'] == tup.id2,'product'].array[0]\n",
        "        if product1 != product2:\n",
        "            df.drop([tup.Index],inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npiq2Y2UuJhY",
        "colab_type": "text"
      },
      "source": [
        "Model-1: Similarity Score - Word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "It9SV878P1JL",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# Calculate similarity score Using word2vec\n",
        "start_time = time.time()\n",
        "\n",
        "# Convert short_desc str to nlp format to fasten the computation process\n",
        "processed_data_nlp = []\n",
        "\n",
        "for tup in processed_data_df.itertuples():\n",
        "    processed_data_nlp.append((tup.id,tup.product,nlp(tup.short_desc)))\n",
        "    \n",
        "processed_data_nlp_df = pd.DataFrame(processed_data_nlp, columns=['id','product','short_desc'])\n",
        "processed_data_nlp_df.head()\n",
        "del processed_data_nlp\n",
        "\n",
        "# Calculate the similarity score\n",
        "similarities_score_list = []\n",
        "\n",
        "for doc1 in processed_data_nlp_df.itertuples():\n",
        "    for doc2 in processed_data_nlp_df.itertuples():\n",
        "        if (doc1.id < doc2.id) and (doc1.product == doc2.product): # if two bug reports belong to the same product then check the similarity\n",
        "            similarity_score = doc1.short_desc.similarity(doc2.short_desc)\n",
        "            similarities_score_list.append((doc1.id,doc2.id,similarity_score))         \n",
        "\n",
        "# Sort based on the score and \n",
        "sorted_similarities_score = sorted(similarities_score_list, key=lambda tup: tup[2], reverse=True)\n",
        "del similarities_score_list\n",
        "\n",
        "#convert to dataframe\n",
        "word2vec_similarities_score_df = pd.DataFrame(sorted_similarities_score, columns=['id1','id2','score'] )\n",
        "del sorted_similarities_score\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time)) # show the time of process\n",
        "word2vec_similarities_score_df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jw08fd9KAFuA",
        "colab": {}
      },
      "source": [
        "Model-2: Similarity Score - tf-idf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHiTYERxuJhf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "start_time = time.time()\n",
        "\n",
        "X_train = processed_data_df['short_desc']\n",
        "\n",
        "# Vectorization \n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf = vectorizer.fit_transform(X_train) # remember to use the original X_train set\n",
        "\n",
        "# Calculate the similarity score\n",
        "tfidf_cosine_similarities = linear_kernel(tfidf)\n",
        "\n",
        "shape = tfidf_cosine_similarities.shape[0]\n",
        "\n",
        "tfidf_cosine_similarities_list = []\n",
        "\n",
        "for index1 in range(shape):\n",
        "    for index2 in range(shape):\n",
        "        if index2 > index1:\n",
        "            tfidf_cosine_similarities_list.append([index1,index2,tfidf_cosine_similarities[index1,index2]]) \n",
        "    \n",
        "# Sort the score list\n",
        "tfidf_cosine_similarities_sort = sorted(tfidf_cosine_similarities_list, key=lambda tup: tup[2], reverse=True)\n",
        "del tfidf_cosine_similarities_list\n",
        "\n",
        "#Conver to dataframe\n",
        "tfidf_cosine_similarities_score_df = pd.DataFrame(tfidf_cosine_similarities_sort, columns=['id1','id2','score'])\n",
        "del tfidf_cosine_similarities_sort\n",
        "\n",
        "# Convert index to id\n",
        "index_to_id(tfidf_cosine_similarities_score_df)\n",
        "\n",
        "# Remove record of diff product\n",
        "remove_diff_product_score(tfidf_cosine_similarities_score_df)\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time)) # show the time of process\n",
        "\n",
        "tfidf_cosine_similarities_score_df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-lLhKuZuJhh",
        "colab_type": "text"
      },
      "source": [
        "Model-3: Similarity Score - BM24F"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5-7vRxwuJhi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "start_time = time.time()\n",
        "\n",
        "# preprocess - tokenize the short_desc to token\n",
        "processed_corpus_list = []\n",
        "for x in processed_data_df.itertuples():\n",
        "    short_desc_splited = x.short_desc.split(\" \")\n",
        "    processed_corpus_list.append(short_desc_splited)\n",
        "    \n",
        "# Create a MB24 Object with the corpus\n",
        "bm25 = BM25Okapi(processed_corpus_list)\n",
        "\n",
        "# Calculat the similarity score for all bug reports\n",
        "BM24_similarity_score_list = []\n",
        "\n",
        "for x in processed_data_df.itertuples():\n",
        "    query = processed_corpus_list[x.Index]\n",
        "    doc_scores = bm25.get_scores(query)\n",
        "    BM24_similarity_score_list.append([x.Index,doc_scores])\n",
        "\n",
        "# Reformating \n",
        "shape = doc_scores.shape[0]\n",
        "BM24_similarity_score_list_2 = []\n",
        "\n",
        "for index1 in range(shape):\n",
        "    for index2 in range(shape):\n",
        "        if index2 > index1:\n",
        "            BM24_similarity_score_list_2.append([index1,index2,BM24_similarity_score_list[index1][1][index2]])  \n",
        "            \n",
        "del BM24_similarity_score_list\n",
        "\n",
        "# Sort the score list\n",
        "BM24_similarity_score_sort = sorted(BM24_similarity_score_list_2, key=lambda tup: tup[2], reverse=True)\n",
        "del BM24_similarity_score_list_2\n",
        "\n",
        "#Conver to dataframe\n",
        "BM24_similarity_score_sort_df = pd.DataFrame(BM24_similarity_score_sort, columns=['id1','id2','score'])\n",
        "del BM24_similarity_score_sort\n",
        "\n",
        "# Convert index to id\n",
        "index_to_id(BM24_similarity_score_sort_df)\n",
        "\n",
        "# Remove record of diff product\n",
        "remove_diff_product_score(BM24_similarity_score_sort_df)\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time)) # show the time of process\n",
        "\n",
        "BM24_similarity_score_sort_df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-4JXZCtuJhl",
        "colab_type": "text"
      },
      "source": [
        "Assessment the Model Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R88un1-uJhm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "duplicate_org = pd.read_csv('https://zenodo.org/record/400614/files/apache.relations.csv?download=1',sep=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6w_IKq3uJhp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2vec_df = word2vec_similarities_score_df\n",
        "tfidf_df = tfidf_cosine_similarities_score_df\n",
        "BM24_df = BM24_similarity_score_sort_df\n",
        "\n",
        "duplicate_score_table_list = []\n",
        "\n",
        "for tup in duplicate_org.itertuples():\n",
        "    word2vec_score = word2vec_df.loc[(word2vec_df['id1'] == tup.id) & (word2vec_df['id2'] == tup.dup),'score'].array[0]\n",
        "    tfidf_score = tfidf_df.loc[(tfidf_df['id1'] == tup.id) & (tfidf_df['id2'] == tup.dup),'score'].array[0]\n",
        "    BM24F_score = BM24_df.loc[(BM24_df['id1'] == tup.id) & (BM24_df['id2'] == tup.dup),'score'].array[0]\n",
        "\n",
        "    short_desc1 = data_org.loc[lambda df: df['id'] == tup.id,'short_desc'].array[0]\n",
        "    short_desc2 = data_org.loc[lambda df: df['id'] == tup.dup,'short_desc'].array[0]\n",
        "    \n",
        "    duplicate_score_table_list.append([tup.id,tup.dup,word2vec_score,tfidf_score,BM24F_score,short_desc1,short_desc2])\n",
        "\n",
        "duplicate_score_table_df = pd.DataFrame(duplicate_score_table_list, columns=['id1','dup','word2vec_score','tfidf_score','BM24F_score','short_desc1','short_desc2'])\n",
        "del duplicate_score_table_list\n",
        "\n",
        "# print(\"short_desc1\\n\",short_desc1)\n",
        "# print(\"short_desc2\\n\",short_desc2)\n",
        "# print(\"word2vec_score:\\t\",word2vec_score)\n",
        "# print(\"tfidf_score:\\t\",tfidf_score)\n",
        "# print(\"BM24F_score:\\t\",BM24F_score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS3qAlqxuJhs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score_table"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCZLd9gsuJhu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrEVs6X-uJhx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN0wN6g9uJh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSR5jqz1uJh6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZHTIEG8uJiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}